{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JVP in Jax\n",
    "\n",
    "JVP computes `J @ v` given a function, evaluation point (primals), and a tangent\n",
    "vector. It does so without computing the full Jacobian (`J`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import make_jaxpr, jvp, linearize\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_fun(x):\n",
    "  a = x ** 2.0\n",
    "  b = a @ jnp.array([[1.0, 2.0]])\n",
    "  c = jnp.sin(b)\n",
    "  d = c @ jnp.array([[1.0, 2.0, 3.0, 4.0], [3.0, 4.0, 5.0, 6.0]])\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-1.8408432, -2.1797118, -2.5185807, -2.8574495], dtype=float32),\n",
       " Array([18.304619, 20.761639, 23.218658, 25.675676], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primals_out, tangents_out = jvp(my_fun, (jnp.array([3.0]), ), (jnp.array([1.0]), ))\n",
    "primals_out, tangents_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Jaxpr below, note that `a` and `b` are tensor constants defined inside `my_fun` that have been\n",
    "hoisted as Jaxpr parameters. `c` is the input primal and `d` is the input tangent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda a:f32[1,2] b:f32[2,4]; c:f32[1] d:f32[1]. let\n",
       "    e:f32[1] = pow c 2.0\n",
       "    f:f32[] = sub 2.0 1.0\n",
       "    g:f32[1] = pow c f\n",
       "    h:f32[1] = mul 2.0 g\n",
       "    i:f32[1] = mul d h\n",
       "    j:f32[2] = dot_general[\n",
       "      dimension_numbers=(([0], [0]), ([], []))\n",
       "      preferred_element_type=float32\n",
       "    ] e a\n",
       "    k:f32[2] = dot_general[\n",
       "      dimension_numbers=(([0], [0]), ([], []))\n",
       "      preferred_element_type=float32\n",
       "    ] i a\n",
       "    l:f32[2] = sin j\n",
       "    m:f32[2] = cos j\n",
       "    n:f32[2] = mul k m\n",
       "    o:f32[4] = dot_general[\n",
       "      dimension_numbers=(([0], [0]), ([], []))\n",
       "      preferred_element_type=float32\n",
       "    ] l b\n",
       "    p:f32[4] = dot_general[\n",
       "      dimension_numbers=(([0], [0]), ([], []))\n",
       "      preferred_element_type=float32\n",
       "    ] n b\n",
       "  in (o, p) }"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(partial(jvp, my_fun))((jnp.array([3.0]), ), (jnp.array([1.0]), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearize in JAX\n",
    "\n",
    "`linearize : (a -> b) -> a -> (b, T a -o T b)`, `linearize` partially evaluates a function given primals and stages out a linear tangent computation.\n",
    "Computation that only depend on primals are evaluated, while computation that depend on tangents are staged out as a Jaxpr.\n",
    "\n",
    "The linear tangent computation is the linear approximation of the function at primals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-1.8408432, -2.1797118, -2.5185807, -2.8574495], dtype=float32),\n",
       " jax.tree_util.Partial(_HashableCallableShim(functools.partial(<function _lift_linearized at 0x7f2a8c03fb00>, { lambda a:f32[1] b:f32[1,2] c:f32[2] d:f32[2,4]; e:f32[1]. let\n",
       "     f:f32[1] = pjit[\n",
       "       name=_power\n",
       "       jaxpr={ lambda ; g:f32[1] h:f32[1]. let i:f32[1] = mul g h in (i,) }\n",
       "     ] e a\n",
       "     j:f32[2] = pjit[\n",
       "       name=matmul\n",
       "       jaxpr={ lambda ; k:f32[1] l:f32[1,2]. let\n",
       "           m:f32[2] = dot_general[\n",
       "             dimension_numbers=(([0], [0]), ([], []))\n",
       "             preferred_element_type=float32\n",
       "           ] k l\n",
       "         in (m,) }\n",
       "     ] f b\n",
       "     n:f32[2] = pjit[\n",
       "       name=sin\n",
       "       jaxpr={ lambda ; o:f32[2] p:f32[2]. let q:f32[2] = mul o p in (q,) }\n",
       "     ] j c\n",
       "     r:f32[4] = pjit[\n",
       "       name=matmul\n",
       "       jaxpr={ lambda ; s:f32[2] t:f32[2,4]. let\n",
       "           u:f32[4] = dot_general[\n",
       "             dimension_numbers=(([0], [0]), ([], []))\n",
       "             preferred_element_type=float32\n",
       "           ] s t\n",
       "         in (u,) }\n",
       "     ] n d\n",
       "   in (r,) }, [ConcreteArray([3.], dtype=float32)], (PyTreeDef((*,)), PyTreeDef(*)), [(ShapedArray(float32[4]), None)])), (Array([6.], dtype=float32), Array([[1., 2.]], dtype=float32), Array([-0.91113025,  0.6603167 ], dtype=float32), Array([[1., 2., 3., 4.],\n",
       "        [3., 4., 5., 6.]], dtype=float32))))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primals_out, linear_jvp = linearize(my_fun, jnp.array([3.0]))\n",
    "primals_out, linear_jvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.304619 20.761639 23.218658 25.675676]\n",
      "{ lambda a:f32[1] b:f32[1,2] c:f32[2] d:f32[2,4]; e:f32[1]. let\n",
      "    f:f32[1] = mul e a\n",
      "    g:f32[2] = dot_general[\n",
      "      dimension_numbers=(([0], [0]), ([], []))\n",
      "      preferred_element_type=float32\n",
      "    ] f b\n",
      "    h:f32[2] = mul g c\n",
      "    i:f32[4] = dot_general[\n",
      "      dimension_numbers=(([0], [0]), ([], []))\n",
      "      preferred_element_type=float32\n",
      "    ] h d\n",
      "  in (i,) }\n"
     ]
    }
   ],
   "source": [
    "print(linear_jvp(jnp.array([1.0])))\n",
    "print(make_jaxpr(linear_jvp)(jnp.array([1.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VJP in JAX\n",
    "\n",
    "Note that the VJP function can be obtained from the JVP function by:\n",
    "\n",
    "- Replacing the tangent vector (`[1.0]`) with the cotangent vector (`[1.0]*4`).\n",
    "\n",
    "- Transposing the linear JVP mapping. That means each primitive operation\n",
    "  (e.g. matmul) is transposed, and the order of operations is reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda a:f32[2,4] b:f32[2] c:f32[1,2] d:f32[1]; e:f32[4]. let\n",
      "    f:f32[2] = dot_general[\n",
      "      dimension_numbers=(([0], [1]), ([], []))\n",
      "      preferred_element_type=float32\n",
      "    ] e a\n",
      "    g:f32[2] = mul f b\n",
      "    h:f32[1] = dot_general[\n",
      "      dimension_numbers=(([0], [1]), ([], []))\n",
      "      preferred_element_type=float32\n",
      "    ] g c\n",
      "    i:f32[1] = mul h d\n",
      "  in (i,) }\n",
      "(Array([87.96059], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "linear_vjp = jax.linear_transpose(linear_jvp, jnp.array([1.0]))\n",
    "print(make_jaxpr(linear_vjp)(jnp.array([1.0, 1.0, 1.0, 1.0])))\n",
    "print(linear_vjp(jnp.array([1.0, 1.0, 1.0, 1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([87.96059], dtype=float32),)\n"
     ]
    }
   ],
   "source": [
    "primals = jnp.array([3.0])\n",
    "print(jax.vjp(my_fun, primals)[1](jnp.array([1.0, 1.0, 1.0, 1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
